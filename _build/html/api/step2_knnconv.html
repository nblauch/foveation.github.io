

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Building foveated deep vision models based on kNN-convolution &mdash; foveation 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=01f34227"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            foveation
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../read_me.html">A biologically-inspired foveated interface for deep vision models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Core Components</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="foveation.sensing.html">foveation.sensing package</a></li>
<li class="toctree-l1"><a class="reference internal" href="foveation.arch.html">foveation.arch package</a></li>
<li class="toctree-l1"><a class="reference internal" href="foveation.saccadenet.html">foveation.saccadenet</a></li>
<li class="toctree-l1"><a class="reference internal" href="foveation.trainer.html">foveation.trainer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Utilities &amp; Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="foveation.utils.html">foveation.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="foveation.demo.html">foveation.demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="foveation.paths.html">foveation.paths</a></li>
<li class="toctree-l1"><a class="reference internal" href="foveation.probes.html">foveation.probes</a></li>
<li class="toctree-l1"><a class="reference internal" href="foveation.visualizer.html">foveation.visualizer</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">foveation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Building foveated deep vision models based on kNN-convolution</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/api/step2_knnconv.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Building-foveated-deep-vision-models-based-on-kNN-convolution">
<h1>Building foveated deep vision models based on kNN-convolution<a class="headerlink" href="#Building-foveated-deep-vision-models-based-on-kNN-convolution" title="Link to this heading"></a></h1>
<p>Next, we will dive into how to do perceptual processing of our foveated sensor outputs, making use of the sensor manifold.</p>
<p>For this, we use k-nearest-neighbor (kNN) receptive fields. In 2-D, receptive fields are specified as <span class="math notranslate nohighlight">\((h,w)\)</span> rectangular grids; on our 3-D manifold, they are specified as kNNs.</p>
<p>The details are described in the paper. Here, we will go through the relevant code modules to see how to build up networks based on kNN-convolution on the foveated sensor manifold</p>
<p>We will now start looking at <code class="docutils literal notranslate"><span class="pre">foveation.arch</span></code>, where all of the architectural features relevant to foveated perceptual processing live.</p>
<p>The building block layers are stored in <code class="docutils literal notranslate"><span class="pre">foveation.arch.knn</span></code>. Let’s use them to build a super simple 1-layer convolutional network, with a convolution layer followed by a pooling layer, normalization, and ReLU.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2

<span class="kn">from</span><span class="w"> </span><span class="nn">foveation.arch.knn</span><span class="w"> </span><span class="kn">import</span> <span class="n">KNNPoolingLayer</span><span class="p">,</span> <span class="n">KNNConvLayer</span><span class="p">,</span> <span class="n">get_in_out_coords</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">foveation.arch.norm</span><span class="w"> </span><span class="kn">import</span> <span class="n">KNNBatchNorm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="n">fov</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">cmf_a</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span>

<span class="n">cartesian_res</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">conv_kernel_cartesian</span> <span class="o">=</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="n">pool_kernel_cartesian</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">conv_stride</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">pool_stride</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">channels</span> <span class="o">=</span> <span class="mi">128</span>

<span class="c1"># determine neighborhood sizes based on target cartesian kernels</span>
<span class="n">k_conv</span> <span class="o">=</span> <span class="n">conv_kernel_cartesian</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">conv_kernel_cartesian</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">k_pool</span> <span class="o">=</span> <span class="n">pool_kernel_cartesian</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">pool_kernel_cartesian</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># set up coordinates based on input resolution and strides</span>
<span class="n">in_cart_res</span> <span class="o">=</span> <span class="n">cartesian_res</span>
<span class="n">sensor_coords</span><span class="p">,</span> <span class="n">conv_coords</span><span class="p">,</span> <span class="n">out_cart_res</span> <span class="o">=</span> <span class="n">get_in_out_coords</span><span class="p">(</span><span class="n">in_cart_res</span><span class="p">,</span> <span class="n">fov</span><span class="p">,</span> <span class="n">cmf_a</span><span class="p">,</span> <span class="n">conv_stride</span><span class="p">,</span> <span class="n">in_cart_res</span><span class="o">=</span><span class="n">in_cart_res</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># the previous layer is the input to the next layer</span>
<span class="n">in_cart_res</span> <span class="o">=</span> <span class="n">out_cart_res</span>
<span class="n">_</span><span class="p">,</span> <span class="n">pool_coords</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_in_out_coords</span><span class="p">(</span><span class="n">in_cart_res</span><span class="p">,</span> <span class="n">fov</span><span class="p">,</span> <span class="n">cmf_a</span><span class="p">,</span> <span class="n">pool_stride</span><span class="p">,</span> <span class="n">in_cart_res</span><span class="o">=</span><span class="n">in_cart_res</span><span class="p">,</span> <span class="n">in_coords</span><span class="o">=</span><span class="n">conv_coords</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">conv_layer</span> <span class="o">=</span> <span class="n">KNNConvLayer</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">k_conv</span><span class="p">,</span> <span class="n">sensor_coords</span><span class="p">,</span> <span class="n">conv_coords</span><span class="p">,</span>
                        <span class="c1">#   ref_frame_side_length=2*conv_kernel_cartesian[0],</span>
                        <span class="n">arch_flag</span><span class="o">=</span><span class="s1">&#39;nnmap_doubleres&#39;</span><span class="p">,</span>
                          <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                          <span class="p">)</span>
<span class="n">pool_layer</span> <span class="o">=</span> <span class="n">KNNPoolingLayer</span><span class="p">(</span><span class="n">k_pool</span><span class="p">,</span> <span class="n">conv_coords</span><span class="p">,</span> <span class="n">pool_coords</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">full_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">conv_layer</span><span class="p">,</span>
    <span class="n">pool_layer</span><span class="p">,</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">KNNBatchNorm</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pool_coords</span><span class="p">),</span> <span class="n">channels</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
found resolution 53 giving 4085 points (desired: 4096)
found resolution 53 giving 4085 points (desired: 4096)
found resolution 26 giving 964 points (desired: 1024)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/opt/homebrew/Caskroom/miniforge/base/envs/knnconv1/lib/python3.13/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:4324.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
</pre></div></div>
</div>
<p>note that the “auto_match_cart_resources=1” attempts to match resources as closely as possible, but cannot be perfect (see print out). the default behavior is to ensure that we select less than or equal to the cartesian equivalent resources.</p>
<p>ok!</p>
<p>let’s create some fake data and pass it through our simple foveation layer</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Our foveated sensor outputs and KNNLayer inputs/outputs are formatted as [batch, num_channels, num_coords]</span>
<span class="n">x_sensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sensor_coords</span><span class="p">)])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">layer_output</span> <span class="o">=</span> <span class="n">full_layer</span><span class="p">(</span><span class="n">x_sensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layer_output</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
torch.Size([64, 128, 964])
</pre></div></div>
</div>
<p>In practice, the <code class="docutils literal notranslate"><span class="pre">KNNAlexNetBlock</span></code> is set up to do exactly what we just did: combine conv, pooling, nonlinearity, and normalization.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">foveation.arch.knnalexnet</span><span class="w"> </span><span class="kn">import</span> <span class="n">KNNAlexNetBlock</span>

<span class="n">block</span> <span class="o">=</span> <span class="n">KNNAlexNetBlock</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">k_conv</span><span class="p">,</span> <span class="n">fov</span><span class="p">,</span> <span class="n">cmf_a</span><span class="p">,</span> <span class="n">cartesian_res</span><span class="p">,</span> <span class="n">conv_stride</span><span class="p">,</span> <span class="n">cart_res</span><span class="o">=</span><span class="n">cartesian_res</span><span class="p">,</span> <span class="n">pool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pool_k</span><span class="o">=</span><span class="n">k_pool</span><span class="p">,</span> <span class="n">pool_stride</span><span class="o">=</span><span class="n">pool_stride</span><span class="p">,</span> <span class="n">norm_type</span><span class="o">=</span><span class="s1">&#39;batch&#39;</span><span class="p">,</span> <span class="n">auto_match_cart_resources</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">arch_flag</span><span class="o">=</span><span class="s1">&#39;nnmap_doubleres&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
found resolution 53 giving 4085 points (desired: 4096)
found resolution 53 giving 4085 points (desired: 4096)
found resolution 53 giving 4085 points (desired: 4096)
found resolution 26 giving 964 points (desired: 1024)
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">block_output</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">x_sensor</span><span class="p">)</span>

<span class="n">block_output</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
torch.Size([64, 128, 964])
</pre></div></div>
</div>
</section>
<section id="Building-an-AlexNet-like-KNN-model">
<h1>Building an AlexNet-like KNN model<a class="headerlink" href="#Building-an-AlexNet-like-KNN-model" title="Link to this heading"></a></h1>
<p>Now that we’ve seen the layers and blocks, we are ready to build a complete KNNAlexNet model. For this, we will just use our wrapper function, and refer you to the code for further detail. The <code class="docutils literal notranslate"><span class="pre">KNNAlexNet</span></code> class builds AlexNet-like models, but is more flexible to different numbers of layers, different kernel sizes, channels dimensions, etc.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">foveation.arch.knnalexnet</span><span class="w"> </span><span class="kn">import</span> <span class="n">KNNAlexNet</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">KNNAlexNet</span><span class="p">(</span>
    <span class="n">cartesian_res</span><span class="p">,</span>
    <span class="mi">3</span><span class="p">,</span>
    <span class="p">[</span><span class="mi">96</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">384</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span> <span class="c1"># channels per layer</span>
    <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="c1"># conv stride per layer</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="c1"># pool after</span>
    <span class="p">[</span><span class="mi">11</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="o">**</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="o">**</span><span class="mi">2</span><span class="p">],</span> <span class="c1"># k per layer</span>
    <span class="n">n_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">out_res</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># no output pooling</span>
    <span class="n">auto_match_cart_resources</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">norm_type</span><span class="o">=</span><span class="s1">&#39;batch&#39;</span><span class="p">,</span>
    <span class="n">arch_flag</span><span class="o">=</span><span class="s1">&#39;doubleres&#39;</span><span class="p">,</span>
    <span class="n">fov</span><span class="o">=</span><span class="n">fov</span><span class="p">,</span>
    <span class="n">cmf_a</span><span class="o">=</span><span class="n">cmf_a</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
found resolution 53 giving 4085 points (desired: 4096)
found resolution 53 giving 4085 points (desired: 4096)
found resolution 13 giving 230 points (desired: 256)
found resolution 13 giving 230 points (desired: 256)
found resolution 13 giving 230 points (desired: 256)
found resolution 13 giving 230 points (desired: 256)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 4 giving 16 points (desired: 16)
no output pooling layer
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
KNNAlexNet(
  (layers): ModuleList(
    (0): KNNAlexNetBlock(
      (conv): KNNConvLayer(
        in_channels=3
        out_channels=96
        k=121
        in_coords=SamplingCoords(length=4085, fov=16, cmf_a=0.5, resolution=53, style=isotropic)
        out_coords=SamplingCoords(length=230, fov=16, cmf_a=0.5, resolution=13, style=isotropic)
        sample_cortex=True
      )
      (norm): KNNBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation): ReLU()
    )
    (1): KNNAlexNetBlock(
      (conv): KNNConvLayer(
        in_channels=96
        out_channels=256
        k=25
        in_coords=SamplingCoords(length=230, fov=16, cmf_a=0.5, resolution=13, style=isotropic)
        out_coords=SamplingCoords(length=230, fov=16, cmf_a=0.5, resolution=13, style=isotropic)
        sample_cortex=True
      )
      (norm): KNNBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation): ReLU()
      (pool): KNNPoolingLayer(
        mode=max
        k=9
        in_coords=SamplingCoords(length=230, fov=16, cmf_a=0.5, resolution=13, style=isotropic)
        out_coords=SamplingCoords(length=60, fov=16, cmf_a=0.5, resolution=7, style=isotropic)
        sample_cortex=True
      )
    )
    (2): KNNAlexNetBlock(
      (conv): KNNConvLayer(
        in_channels=256
        out_channels=384
        k=9
        in_coords=SamplingCoords(length=60, fov=16, cmf_a=0.5, resolution=7, style=isotropic)
        out_coords=SamplingCoords(length=60, fov=16, cmf_a=0.5, resolution=7, style=isotropic)
        sample_cortex=True
      )
      (norm): KNNBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation): ReLU()
    )
    (3): KNNAlexNetBlock(
      (conv): KNNConvLayer(
        in_channels=384
        out_channels=256
        k=27
        in_coords=SamplingCoords(length=60, fov=16, cmf_a=0.5, resolution=7, style=isotropic)
        out_coords=SamplingCoords(length=60, fov=16, cmf_a=0.5, resolution=7, style=isotropic)
        sample_cortex=True
      )
      (norm): KNNBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation): ReLU()
    )
    (4): KNNAlexNetBlock(
      (conv): KNNConvLayer(
        in_channels=256
        out_channels=256
        k=9
        in_coords=SamplingCoords(length=60, fov=16, cmf_a=0.5, resolution=7, style=isotropic)
        out_coords=SamplingCoords(length=60, fov=16, cmf_a=0.5, resolution=7, style=isotropic)
        sample_cortex=True
      )
      (norm): KNNBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activation): ReLU()
      (pool): KNNPoolingLayer(
        mode=max
        k=9
        in_coords=SamplingCoords(length=60, fov=16, cmf_a=0.5, resolution=7, style=isotropic)
        out_coords=SamplingCoords(length=16, fov=16, cmf_a=0.5, resolution=4, style=isotropic)
        sample_cortex=True
      )
    )
  )
  (classifier): Linear(in_features=4096, out_features=1000, bias=True)
)
</pre></div></div>
</div>
</section>
<section id="Building-a-complete-SaccadeNet">
<h1>Building a complete SaccadeNet<a class="headerlink" href="#Building-a-complete-SaccadeNet" title="Link to this heading"></a></h1>
<p>You may have noticed that we have only worked with fake data thus far. Our KNN architectures are designed to work with outputs formatted on the sensor manifold. To get this from images, we need to use a <code class="docutils literal notranslate"><span class="pre">RetinalTransform</span></code> object, or more simply, a <code class="docutils literal notranslate"><span class="pre">SaccadePolicy</span></code> which will also determine our fixations for us. If you forget about these, go back to <code class="docutils literal notranslate"><span class="pre">step1_sampling.ipynb</span></code>.</p>
<p>The last piece of the puzzle is the <code class="docutils literal notranslate"><span class="pre">SaccadeNet</span></code> class which combines a fixation policy with a processing network. Since there are a lot of hyperparameters, here, we specify them all in a neat hierarchical <code class="docutils literal notranslate"><span class="pre">config</span></code>. This is typically specified as a <code class="docutils literal notranslate"><span class="pre">.yaml</span></code> file, and we use <code class="docutils literal notranslate"><span class="pre">hydra</span></code> and <code class="docutils literal notranslate"><span class="pre">omega</span></code> to handle these in our training scripts. Because the config uses inheritance, we initialize and compose it with <code class="docutils literal notranslate"><span class="pre">hydra</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">foveation.saccadenet</span><span class="w"> </span><span class="kn">import</span> <span class="n">SaccadeNet</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">hydra</span><span class="w"> </span><span class="kn">import</span> <span class="n">compose</span><span class="p">,</span> <span class="n">initialize</span>

<span class="c1"># Use hydra/omega to process the hierarchical config, including all defaults</span>
<span class="k">with</span> <span class="n">initialize</span><span class="p">(</span><span class="n">version_base</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">config_path</span><span class="o">=</span><span class="s2">&quot;../config&quot;</span><span class="p">):</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">compose</span><span class="p">(</span><span class="n">config_name</span><span class="o">=</span><span class="s2">&quot;knnalexnet.yaml&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">config</span><span class="p">))</span> <span class="c1"># OmegaConf DictConfig</span>

<span class="n">saccadenet</span> <span class="o">=</span> <span class="n">SaccadeNet</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;class &#39;omegaconf.dictconfig.DictConfig&#39;&gt;
[[96, 11, 2, 5, 1], [256, 5, 1, 2, 1], [384, 3, 1, 1, 1], [384, 3, 1, 1, 1], [256, 3, 1, 1, 1]]
adjusting FOV for fixation: 16.0 (full: 16.0)
found resolution 53 giving 4085 points (desired: 4096)
found resolution 53 giving 4085 points (desired: 4096)
found resolution 26 giving 964 points (desired: 1024)
found resolution 26 giving 964 points (desired: 1024)
found resolution 13 giving 230 points (desired: 256)
found resolution 13 giving 230 points (desired: 256)
found resolution 13 giving 230 points (desired: 256)
found resolution 13 giving 230 points (desired: 256)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 4 giving 16 points (desired: 16)
found resolution 1 giving 1 points (desired: 1)
WARNING: horizontal flip always done in the loader, regardless of where other transforms are done
loader_transforms: Compose(
    ToTorchImage(device=cpu, dtype=torch.float32, from_numpy=True)
    RandomHorizontalFlip(p=0.5, seed=None)
)
pre_transforms: Compose(
    RandomColorJitter(p=0.8, hue=[-0.1, 0.1], saturation=[0.8, 1.2], value=[0.6, 1.4], contrast=[0.6, 1.4], seed=None)
    RandomGrayscale(p=0.2, num_output_channels=3, seed=None)
    NormalizeGPU(mean=tensor([0.4850, 0.4560, 0.4060], dtype=torch.float64), std=tensor([0.2290, 0.2240, 0.2250], dtype=torch.float64), inplace=True)
)
post_transforms: None
found resolution 53 giving 4085 points (desired: 4096)
Auto-matched resolution to 53 (4085 sampling coordinates) to best match 4096 cartesian pixels.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/Users/nblauch/git/foveation-private/foveation/sensing/coords.py:327: RuntimeWarning: divide by zero encountered in scalar divide
  w_delta = (w_max - w_min)/(res-1)
/Users/nblauch/git/foveation-private/foveation/sensing/retina.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  fix_loc = torch.tensor(self._check_fix_loc(fix_loc, x.shape[0]), dtype=self.dtype, device=self.device)
/opt/homebrew/Caskroom/miniforge/base/envs/knnconv1/lib/python3.13/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of &#39;cuda&#39;, but CUDA is not available. Disabling
  warnings.warn(
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
ssl_fixator:
NoSaccadePolicy(retinal_transform=RetinalTransform(
  (foveal_color): GaussianColorDecay(sigma=None)
  (sampler): GridSampler(fov=16.0, cmf_a=0.5, style=isotropic, resolution=53, mode=nearest, n=4085)
))

sup_fixator:
MultiRandomSaccadePolicy(retinal_transform=RetinalTransform(
  (foveal_color): GaussianColorDecay(sigma=None)
  (sampler): GridSampler(fov=16.0, cmf_a=0.5, style=isotropic, resolution=53, mode=nearest, n=4085)
), n_fixations=4)

LINEAR PROBE NUM CLASSES: 1000
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">saccadenet</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
SaccadeNet(
  (network): BackboneProjectorWrapper(
    (backbone): KNNAlexNet(
      (layers): ModuleList(
        (0): KNNAlexNetBlock(
          (conv): KNNConvLayer(
                in_channels=3
                out_channels=96
                k=121
                in_coords=SamplingCoords(length=4085, fov=16.0, cmf_a=0.5, resolution=53, style=isotropic)
                out_coords=SamplingCoords(length=964, fov=16.0, cmf_a=0.5, resolution=26, style=isotropic)
                sample_cortex=True
          )
          (norm): KNNBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (pool): KNNPoolingLayer(
                mode=max
                k=9
                in_coords=SamplingCoords(length=964, fov=16.0, cmf_a=0.5, resolution=26, style=isotropic)
                out_coords=SamplingCoords(length=230, fov=16.0, cmf_a=0.5, resolution=13, style=isotropic)
                sample_cortex=True
          )
        )
        (1): KNNAlexNetBlock(
          (conv): KNNConvLayer(
                in_channels=96
                out_channels=256
                k=25
                in_coords=SamplingCoords(length=230, fov=16.0, cmf_a=0.5, resolution=13, style=isotropic)
                out_coords=SamplingCoords(length=230, fov=16.0, cmf_a=0.5, resolution=13, style=isotropic)
                sample_cortex=True
          )
          (norm): KNNBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (pool): KNNPoolingLayer(
                mode=max
                k=9
                in_coords=SamplingCoords(length=230, fov=16.0, cmf_a=0.5, resolution=13, style=isotropic)
                out_coords=SamplingCoords(length=60, fov=16.0, cmf_a=0.5, resolution=7, style=isotropic)
                sample_cortex=True
          )
        )
        (2): KNNAlexNetBlock(
          (conv): KNNConvLayer(
                in_channels=256
                out_channels=384
                k=9
                in_coords=SamplingCoords(length=60, fov=16.0, cmf_a=0.5, resolution=7, style=isotropic)
                out_coords=SamplingCoords(length=60, fov=16.0, cmf_a=0.5, resolution=7, style=isotropic)
                sample_cortex=True
          )
          (norm): KNNBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (3): KNNAlexNetBlock(
          (conv): KNNConvLayer(
                in_channels=384
                out_channels=384
                k=9
                in_coords=SamplingCoords(length=60, fov=16.0, cmf_a=0.5, resolution=7, style=isotropic)
                out_coords=SamplingCoords(length=60, fov=16.0, cmf_a=0.5, resolution=7, style=isotropic)
                sample_cortex=True
          )
          (norm): KNNBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
        )
        (4): KNNAlexNetBlock(
          (conv): KNNConvLayer(
                in_channels=384
                out_channels=256
                k=9
                in_coords=SamplingCoords(length=60, fov=16.0, cmf_a=0.5, resolution=7, style=isotropic)
                out_coords=SamplingCoords(length=60, fov=16.0, cmf_a=0.5, resolution=7, style=isotropic)
                sample_cortex=True
          )
          (norm): KNNBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (pool): KNNPoolingLayer(
                mode=max
                k=9
                in_coords=SamplingCoords(length=60, fov=16.0, cmf_a=0.5, resolution=7, style=isotropic)
                out_coords=SamplingCoords(length=16, fov=16.0, cmf_a=0.5, resolution=4, style=isotropic)
                sample_cortex=True
          )
        )
        (5): KNNPoolingLayer(
                mode=avg
                k=16
                in_coords=SamplingCoords(length=16, fov=16.0, cmf_a=0.5, resolution=4, style=isotropic)
                out_coords=SamplingCoords(length=1, fov=16.0, cmf_a=0.5, resolution=1, style=isotropic)
                sample_cortex=True
        )
      )
    )
    (projector): MLPWrapper(
      (layers): Sequential(
        (fc_block_6): LayerBlock(
          (0): Dropout(p=0.5, inplace=False)
          (1): Linear(in_features=256, out_features=1024, bias=True)
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU(inplace=True)
        )
        (fc_block_7): LayerBlock(
          (0): Dropout(p=0.5, inplace=False)
          (1): Linear(in_features=1024, out_features=1024, bias=False)
        )
      )
    )
  )
  (retinal_transform): RetinalTransform(
    (foveal_color): GaussianColorDecay(sigma=None)
    (sampler): GridSampler(fov=16.0, cmf_a=0.5, style=isotropic, resolution=53, mode=nearest, n=4085)
  )
  (ssl_fixator): NoSaccadePolicy(retinal_transform=RetinalTransform(
    (foveal_color): GaussianColorDecay(sigma=None)
    (sampler): GridSampler(fov=16.0, cmf_a=0.5, style=isotropic, resolution=53, mode=nearest, n=4085)
  ))
  (sup_fixator): MultiRandomSaccadePolicy(retinal_transform=RetinalTransform(
    (foveal_color): GaussianColorDecay(sigma=None)
    (sampler): GridSampler(fov=16.0, cmf_a=0.5, style=isotropic, resolution=53, mode=nearest, n=4085)
  ), n_fixations=4)
  (head): SaccadeNetProbe(
    (fix_projector): LinearProbe(
      (dropout): Dropout(p=0.5, inplace=False)
      (probe): Linear(in_features=1024, out_features=1000, bias=True)
    )
  )
)
</pre></div></div>
</div>
<p>We can now process image data with our saccadenet model</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">foveation.demo</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_image_as_batch</span>

<span class="c1"># data = torch.rand([128, 3, 256, 256]).to(device)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">get_image_as_batch</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">category_logits</span><span class="p">,</span> <span class="n">layer_outputs</span><span class="p">,</span> <span class="n">x_fixs</span> <span class="o">=</span> <span class="n">saccadenet</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
torch.Size([1, 3, 256, 256])
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/Users/nblauch/git/foveation-private/foveation/sensing/retina.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  fix_loc = torch.tensor(self._check_fix_loc(fix_loc, x.shape[0]), dtype=self.dtype, device=self.device)
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># global avg pool of final conv layer</span>
<span class="nb">print</span><span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># (batch, num_fixations, conv_dim)</span>
<span class="c1"># final MLP layer:</span>
<span class="nb">print</span><span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># (batch, num_fixations, fc_dim)</span>
<span class="c1"># category logits averaged across fixations</span>
<span class="nb">print</span><span class="p">(</span><span class="n">category_logits</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># (batch, num_classes)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
torch.Size([1, 4, 256])
torch.Size([1, 4, 1024])
torch.Size([1, 1000])
</pre></div></div>
</div>
</section>
<section id="load-a-pre-trained-model">
<h1>load a pre-trained model<a class="headerlink" href="#load-a-pre-trained-model" title="Link to this heading"></a></h1>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">foveation</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_config</span>

<span class="n">base_fn</span> <span class="o">=</span> <span class="s1">&#39;fovknnalexnet_a-1_res-64_in1k&#39;</span>
<span class="n">config</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">model_key</span> <span class="o">=</span> <span class="n">load_config</span><span class="p">(</span><span class="n">base_fn</span><span class="p">,</span> <span class="n">load</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">folder</span><span class="o">=</span><span class="s1">&#39;../models&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SaccadeNet</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="n">model_key</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[96, 11, 2, 5, 1], [256, 5, 1, 2, 1], [384, 3, 1, 1, 1], [384, 3, 1, 1, 1], [256, 3, 1, 1, 1]]
adjusting FOV for fixation: 16.0 (full: 16.0)
found resolution 53 giving 4085 points (desired: 4096)
found resolution 53 giving 4085 points (desired: 4096)
found resolution 26 giving 964 points (desired: 1024)
found resolution 26 giving 964 points (desired: 1024)
found resolution 13 giving 230 points (desired: 256)
found resolution 13 giving 230 points (desired: 256)
found resolution 13 giving 230 points (desired: 256)
found resolution 13 giving 230 points (desired: 256)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 7 giving 60 points (desired: 64)
found resolution 4 giving 16 points (desired: 16)
found resolution 1 giving 1 points (desired: 1)
WARNING: horizontal flip always done in the loader, regardless of where other transforms are done
loader_transforms: Compose(
    ToTorchImage(device=cpu, dtype=torch.float32, from_numpy=True)
    RandomHorizontalFlip(p=0.5, seed=None)
)
pre_transforms: Compose(
    RandomColorJitter(p=0.8, hue=[-0.1, 0.1], saturation=[0.8, 1.2], value=[0.6, 1.4], contrast=[0.6, 1.4], seed=None)
    RandomGrayscale(p=0.2, num_output_channels=3, seed=None)
    NormalizeGPU(mean=tensor([0.4850, 0.4560, 0.4060], dtype=torch.float64), std=tensor([0.2290, 0.2240, 0.2250], dtype=torch.float64), inplace=True)
)
post_transforms: None
found resolution 53 giving 4085 points (desired: 4096)
Auto-matched resolution to 53 (4085 sampling coordinates) to best match 4096 cartesian pixels.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/Users/nblauch/git/foveation-private/foveation/sensing/coords.py:327: RuntimeWarning: divide by zero encountered in scalar divide
  w_delta = (w_max - w_min)/(res-1)
/opt/homebrew/Caskroom/miniforge/base/envs/knnconv1/lib/python3.13/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of &#39;cuda&#39;, but CUDA is not available. Disabling
  warnings.warn(
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
ssl_fixator:
NoSaccadePolicy(retinal_transform=RetinalTransform(
  (foveal_color): GaussianColorDecay(sigma=None)
  (sampler): GridSampler(fov=16.0, cmf_a=0.5, style=isotropic, resolution=53, mode=nearest, n=4085)
))

sup_fixator:
MultiRandomSaccadePolicy(retinal_transform=RetinalTransform(
  (foveal_color): GaussianColorDecay(sigma=None)
  (sampler): GridSampler(fov=16.0, cmf_a=0.5, style=isotropic, resolution=53, mode=nearest, n=4085)
), n_fixations=4)

LINEAR PROBE NUM CLASSES: 1000
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;All keys matched successfully&gt;
</pre></div></div>
</div>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>
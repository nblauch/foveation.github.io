{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6d7909",
   "metadata": {},
   "source": [
    "# Get activations from a foveated model\n",
    "\n",
    "Here we will demonstrate two methods for getting activitations. The first uses the model class directly. \n",
    "\n",
    "Let's load a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5424f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[96, 11, 2, 5, 1], [256, 5, 1, 2, 1], [384, 3, 1, 1, 1], [384, 3, 1, 1, 1], [256, 3, 1, 1, 1]]\n",
      "adjusting FOV for fixation: 16.0 (full: 16.0)\n",
      "found resolution 53 giving 4085 points (desired: 4096)\n",
      "found resolution 53 giving 4085 points (desired: 4096)\n",
      "found resolution 26 giving 964 points (desired: 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/alvarez_lab_tier1/Users/nblauch/conda_envs/new_workshop/lib/python3.9/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found resolution 26 giving 964 points (desired: 1024)\n",
      "found resolution 13 giving 230 points (desired: 256)\n",
      "found resolution 13 giving 230 points (desired: 256)\n",
      "found resolution 13 giving 230 points (desired: 256)\n",
      "found resolution 13 giving 230 points (desired: 256)\n",
      "found resolution 7 giving 60 points (desired: 64)\n",
      "found resolution 7 giving 60 points (desired: 64)\n",
      "found resolution 7 giving 60 points (desired: 64)\n",
      "found resolution 7 giving 60 points (desired: 64)\n",
      "found resolution 7 giving 60 points (desired: 64)\n",
      "found resolution 7 giving 60 points (desired: 64)\n",
      "found resolution 7 giving 60 points (desired: 64)\n",
      "found resolution 7 giving 60 points (desired: 64)\n",
      "found resolution 4 giving 16 points (desired: 16)\n",
      "found resolution 1 giving 1 points (desired: 1)\n",
      "WARNING: horizontal flip always done in the loader, regardless of where other transforms are done\n",
      "loader_transforms: Compose(\n",
      "    ToTorchImage(device=cpu, dtype=torch.float32, from_numpy=True)\n",
      "    RandomHorizontalFlip(p=0.5, seed=None)\n",
      ")\n",
      "pre_transforms: Compose(\n",
      "    RandomColorJitter(p=0.8, hue=[-0.1, 0.1], saturation=[0.8, 1.2], value=[0.6, 1.4], contrast=[0.6, 1.4], seed=None)\n",
      "    RandomGrayscale(p=0.2, num_output_channels=3, seed=None)\n",
      "    NormalizeGPU(mean=tensor([0.4850, 0.4560, 0.4060], dtype=torch.float64), std=tensor([0.2290, 0.2240, 0.2250], dtype=torch.float64), inplace=True)\n",
      ")\n",
      "post_transforms: None\n",
      "found resolution 53 giving 4085 points (desired: 4096)\n",
      "Auto-matched resolution to 53 (4085 sampling coordinates) to best match 4096 cartesian pixels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/nblauch/git/foveation-private/foveation/sensing/coords.py:327: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  w_delta = (w_max - w_min)/(res-1)\n",
      "/n/home12/nblauch/git/foveation-private/foveation/sensing/retina.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fix_loc = torch.tensor(self._check_fix_loc(fix_loc, x.shape[0]), dtype=self.dtype, device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssl_fixator:\n",
      "NoSaccadePolicy(retinal_transform=RetinalTransform(\n",
      "  (foveal_color): GaussianColorDecay(sigma=None)\n",
      "  (sampler): GridSampler(fov=16.0, cmf_a=0.5, style=isotropic, resolution=53, mode=nearest, n=4085)\n",
      "))\n",
      "\n",
      "sup_fixator:\n",
      "MultiRandomSaccadePolicy(retinal_transform=RetinalTransform(\n",
      "  (foveal_color): GaussianColorDecay(sigma=None)\n",
      "  (sampler): GridSampler(fov=16.0, cmf_a=0.5, style=isotropic, resolution=53, mode=nearest, n=4085)\n",
      "), n_fixations=4)\n",
      "\n",
      "LINEAR PROBE NUM CLASSES: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from foveation import load_config\n",
    "from foveation.saccadenet import SaccadeNet\n",
    "\n",
    "base_fn = 'fovknnalexnet_a-1_res-64_in1k'\n",
    "config, state_dict, model_key = load_config(base_fn, load=True, folder='../models', device='cpu')\n",
    "model = SaccadeNet(config, device='cpu')\n",
    "model.load_state_dict(state_dict[model_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66eca93",
   "metadata": {},
   "source": [
    "### Now we can create some fake data and get activations.\n",
    "First, let's see which layers are available to hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f6fbca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'backbone',\n",
       " 'backbone.layers',\n",
       " 'backbone.layers.0',\n",
       " 'backbone.layers.0.conv',\n",
       " 'backbone.layers.0.conv.conv',\n",
       " 'backbone.layers.0.norm',\n",
       " 'backbone.layers.0.activation',\n",
       " 'backbone.layers.0.pool',\n",
       " 'backbone.layers.1',\n",
       " 'backbone.layers.1.conv',\n",
       " 'backbone.layers.1.conv.conv',\n",
       " 'backbone.layers.1.norm',\n",
       " 'backbone.layers.1.activation',\n",
       " 'backbone.layers.1.pool',\n",
       " 'backbone.layers.2',\n",
       " 'backbone.layers.2.conv',\n",
       " 'backbone.layers.2.conv.conv',\n",
       " 'backbone.layers.2.norm',\n",
       " 'backbone.layers.2.activation',\n",
       " 'backbone.layers.3',\n",
       " 'backbone.layers.3.conv',\n",
       " 'backbone.layers.3.conv.conv',\n",
       " 'backbone.layers.3.norm',\n",
       " 'backbone.layers.3.activation',\n",
       " 'backbone.layers.4',\n",
       " 'backbone.layers.4.conv',\n",
       " 'backbone.layers.4.conv.conv',\n",
       " 'backbone.layers.4.norm',\n",
       " 'backbone.layers.4.activation',\n",
       " 'backbone.layers.4.pool',\n",
       " 'backbone.layers.5',\n",
       " 'projector',\n",
       " 'projector.layers',\n",
       " 'projector.layers.fc_block_6',\n",
       " 'projector.layers.fc_block_6.0',\n",
       " 'projector.layers.fc_block_6.1',\n",
       " 'projector.layers.fc_block_6.2',\n",
       " 'projector.layers.fc_block_6.3',\n",
       " 'projector.layers.fc_block_7',\n",
       " 'projector.layers.fc_block_7.0',\n",
       " 'projector.layers.fc_block_7.1']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.list_available_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c859038f",
   "metadata": {},
   "source": [
    "Let's hook the the fourth backbone block (layers.3), the full backbone (conv layers), and the projector (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d070ab0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/nblauch/git/foveation-private/foveation/sensing/retina.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fix_loc = torch.tensor(self._check_fix_loc(fix_loc, x.shape[0]), dtype=self.dtype, device=self.device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.rand((10, 3, 256, 256)).to('cpu')\n",
    "outputs, acts = model.get_activations(inputs, layer_names=['backbone.layers.3', 'backbone', 'projector'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18daf1a9",
   "metadata": {},
   "source": [
    "Note that the intermediate backbone block retains a spatial dimension ($n=60$), whereas the full backbone has been globally pooled and has no spatial dimension, similarly to the projector.\n",
    "\n",
    "Note also that each activation tensor contains a fixation dimension as the second dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da0a2d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'backbone.layers.3': torch.Size([10, 4, 384, 60]),\n",
       " 'backbone': torch.Size([10, 4, 256]),\n",
       " 'projector': torch.Size([10, 4, 1024])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v.shape for k, v in acts.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508746f5",
   "metadata": {},
   "source": [
    "# Using the trainer class\n",
    "\n",
    "An even more stream-lined way of getting activations is to use the Trainer class. \n",
    "\n",
    "For this to work, you will need to define paths to existing dataset files. For now, these must be FFCV files. Soon, we will allow for standard image datasets. \n",
    "\n",
    "When loading a trainer from pre-trained, it is generally easiest to use the utility `get_trainer_from_base_fn`, which does a few basic things under the hood so we don't need to manually edit the config to turn off distributed training, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f1204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[96, 11, 2, 5, 1], [256, 5, 1, 2, 1], [384, 3, 1, 1, 1], [384, 3, 1, 1, 1], [256, 3, 1, 1, 1]]\n",
      "adjusting FOV for fixation: 16.0 (full: 16.0)\n",
      "found resolution 53 giving 4085 points (desired: 4096)\n",
      "found resolution 53 giving 4085 points (desired: 4096)\n",
      "found resolution 26 giving 964 points (desired: 1024)\n",
      "found resolution 26 giving 964 points (desired: 1024)\n",
      "found resolution 13 giving 230 points (desired: 256)\n",
      "found resolution 13 giving 230 points (desired: 256)\n",
      "found resolution 13 giving 230 points (desired: 256)\n",
      "found resolution 13 giving 230 points (desired: 256)\n",
      "found resolution 7 giving 60 points (desired: 64)\n",
      "found resolution 7 giving 60 points (desired: 64)\n",
      "found resolution 7 giving 60 points (desired: 64)\n",
      "found resolution 7 giving 60 points (desired: 64)\n",
      "found resolution 7 giving 60 points (desired: 64)\n",
      "found resolution 7 giving 60 points (desired: 64)\n",
      "found resolution 7 giving 60 points (desired: 64)\n",
      "found resolution 7 giving 60 points (desired: 64)\n",
      "found resolution 4 giving 16 points (desired: 16)\n",
      "found resolution 1 giving 1 points (desired: 1)\n",
      "WARNING: horizontal flip always done in the loader, regardless of where other transforms are done\n",
      "loader_transforms: Compose(\n",
      "    ToTorchImage(device=cuda, dtype=torch.float32, from_numpy=True)\n",
      "    RandomHorizontalFlip(p=0.5, seed=None)\n",
      ")\n",
      "pre_transforms: Compose(\n",
      "    RandomColorJitter(p=0.8, hue=[-0.1, 0.1], saturation=[0.8, 1.2], value=[0.6, 1.4], contrast=[0.6, 1.4], seed=None)\n",
      "    RandomGrayscale(p=0.2, num_output_channels=3, seed=None)\n",
      "    NormalizeGPU(mean=tensor([0.4850, 0.4560, 0.4060], device='cuda:0', dtype=torch.float64), std=tensor([0.2290, 0.2240, 0.2250], device='cuda:0', dtype=torch.float64), inplace=True)\n",
      ")\n",
      "post_transforms: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/nblauch/git/foveation-private/foveation/sensing/coords.py:327: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  w_delta = (w_max - w_min)/(res-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found resolution 53 giving 4085 points (desired: 4096)\n",
      "Auto-matched resolution to 53 (4085 sampling coordinates) to best match 4096 cartesian pixels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home12/nblauch/git/foveation-private/foveation/sensing/retina.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fix_loc = torch.tensor(self._check_fix_loc(fix_loc, x.shape[0]), dtype=self.dtype, device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssl_fixator:\n",
      "NoSaccadePolicy(retinal_transform=RetinalTransform(\n",
      "  (foveal_color): GaussianColorDecay(sigma=None)\n",
      "  (sampler): GridSampler(fov=16.0, cmf_a=0.5, style=isotropic, resolution=53, mode=nearest, n=4085)\n",
      "))\n",
      "\n",
      "sup_fixator:\n",
      "MultiRandomSaccadePolicy(retinal_transform=RetinalTransform(\n",
      "  (foveal_color): GaussianColorDecay(sigma=None)\n",
      "  (sampler): GridSampler(fov=16.0, cmf_a=0.5, style=isotropic, resolution=53, mode=nearest, n=4085)\n",
      "), n_fixations=4)\n",
      "\n",
      "LINEAR PROBE NUM CLASSES: 1000\n",
      "SaccadeNet(\n",
      "  (network): BackboneProjectorWrapper(\n",
      "    (backbone): KNNAlexNet(\n",
      "      (layers): ModuleList(\n",
      "        (0): KNNAlexNetBlock(\n",
      "          (conv): KNNConvLayer(\n",
      "          \tin_channels=3\n",
      "          \tout_channels=96\n",
      "          \tk=121\n",
      "          \tin_coords=SamplingCoords(length=4085, fov=16.0, cmf_a=0.5, resolution=53, style=isotropic)\n",
      "          \tout_coords=SamplingCoords(length=964, fov=16.0, cmf_a=0.5, resolution=26, style=isotropic)\n",
      "          \tsample_cortex=True\n",
      "          )\n",
      "          (norm): KNNBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (pool): KNNPoolingLayer(\n",
      "          \tmode=max\n",
      "          \tk=9\n",
      "          \tin_coords=SamplingCoords(length=964, fov=16.0, cmf_a=0.5, resolution=26, style=isotropic)\n",
      "          \tout_coords=SamplingCoords(length=230, fov=16.0, cmf_a=0.5, resolution=13, style=isotropic)\n",
      "          \tsample_cortex=True\n",
      "          )\n",
      "        )\n",
      "        (1): KNNAlexNetBlock(\n",
      "          (conv): KNNConvLayer(\n",
      "          \tin_channels=96\n",
      "          \tout_channels=256\n",
      "          \tk=25\n",
      "          \tin_coords=SamplingCoords(length=230, fov=16.0, cmf_a=0.5, resolution=13, style=isotropic)\n",
      "          \tout_coords=SamplingCoords(length=230, fov=16.0, cmf_a=0.5, resolution=13, style=isotropic)\n",
      "          \tsample_cortex=True\n",
      "          )\n",
      "          (norm): KNNBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (pool): KNNPoolingLayer(\n",
      "          \tmode=max\n",
      "          \tk=9\n",
      "          \tin_coords=SamplingCoords(length=230, fov=16.0, cmf_a=0.5, resolution=13, style=isotropic)\n",
      "          \tout_coords=SamplingCoords(length=60, fov=16.0, cmf_a=0.5, resolution=7, style=isotropic)\n",
      "          \tsample_cortex=True\n",
      "          )\n",
      "        )\n",
      "        (2): KNNAlexNetBlock(\n",
      "          (conv): KNNConvLayer(\n",
      "          \tin_channels=256\n",
      "          \tout_channels=384\n",
      "          \tk=9\n",
      "          \tin_coords=SamplingCoords(length=60, fov=16.0, cmf_a=0.5, resolution=7, style=isotropic)\n",
      "          \tout_coords=SamplingCoords(length=60, fov=16.0, cmf_a=0.5, resolution=7, style=isotropic)\n",
      "          \tsample_cortex=True\n",
      "          )\n",
      "          (norm): KNNBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "        (3): KNNAlexNetBlock(\n",
      "          (conv): KNNConvLayer(\n",
      "          \tin_channels=384\n",
      "          \tout_channels=384\n",
      "          \tk=9\n",
      "          \tin_coords=SamplingCoords(length=60, fov=16.0, cmf_a=0.5, resolution=7, style=isotropic)\n",
      "          \tout_coords=SamplingCoords(length=60, fov=16.0, cmf_a=0.5, resolution=7, style=isotropic)\n",
      "          \tsample_cortex=True\n",
      "          )\n",
      "          (norm): KNNBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "        (4): KNNAlexNetBlock(\n",
      "          (conv): KNNConvLayer(\n",
      "          \tin_channels=384\n",
      "          \tout_channels=256\n",
      "          \tk=9\n",
      "          \tin_coords=SamplingCoords(length=60, fov=16.0, cmf_a=0.5, resolution=7, style=isotropic)\n",
      "          \tout_coords=SamplingCoords(length=60, fov=16.0, cmf_a=0.5, resolution=7, style=isotropic)\n",
      "          \tsample_cortex=True\n",
      "          )\n",
      "          (norm): KNNBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "          (pool): KNNPoolingLayer(\n",
      "          \tmode=max\n",
      "          \tk=9\n",
      "          \tin_coords=SamplingCoords(length=60, fov=16.0, cmf_a=0.5, resolution=7, style=isotropic)\n",
      "          \tout_coords=SamplingCoords(length=16, fov=16.0, cmf_a=0.5, resolution=4, style=isotropic)\n",
      "          \tsample_cortex=True\n",
      "          )\n",
      "        )\n",
      "        (5): KNNPoolingLayer(\n",
      "        \tmode=avg\n",
      "        \tk=16\n",
      "        \tin_coords=SamplingCoords(length=16, fov=16.0, cmf_a=0.5, resolution=4, style=isotropic)\n",
      "        \tout_coords=SamplingCoords(length=1, fov=16.0, cmf_a=0.5, resolution=1, style=isotropic)\n",
      "        \tsample_cortex=True\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (projector): MLPWrapper(\n",
      "      (layers): Sequential(\n",
      "        (fc_block_6): LayerBlock(\n",
      "          (0): Dropout(p=0.5, inplace=False)\n",
      "          (1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): ReLU(inplace=True)\n",
      "        )\n",
      "        (fc_block_7): LayerBlock(\n",
      "          (0): Dropout(p=0.5, inplace=False)\n",
      "          (1): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (retinal_transform): RetinalTransform(\n",
      "    (foveal_color): GaussianColorDecay(sigma=None)\n",
      "    (sampler): GridSampler(fov=16.0, cmf_a=0.5, style=isotropic, resolution=53, mode=nearest, n=4085)\n",
      "  )\n",
      "  (ssl_fixator): NoSaccadePolicy(retinal_transform=RetinalTransform(\n",
      "    (foveal_color): GaussianColorDecay(sigma=None)\n",
      "    (sampler): GridSampler(fov=16.0, cmf_a=0.5, style=isotropic, resolution=53, mode=nearest, n=4085)\n",
      "  ))\n",
      "  (sup_fixator): MultiRandomSaccadePolicy(retinal_transform=RetinalTransform(\n",
      "    (foveal_color): GaussianColorDecay(sigma=None)\n",
      "    (sampler): GridSampler(fov=16.0, cmf_a=0.5, style=isotropic, resolution=53, mode=nearest, n=4085)\n",
      "  ), n_fixations=4)\n",
      "  (head): SaccadeNetProbe(\n",
      "    (fix_projector): LinearProbe(\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (probe): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "NUM PROBE LAYERS: 3\n",
      "LINEAR PROBE NUM CLASSES: 1000\n",
      "LINEAR PROBE NUM CLASSES: 1000\n",
      "LINEAR PROBE NUM CLASSES: 1000\n",
      "just resizing, no crops\n",
      "output_size: (256, 256), scale: (1.0, 1.0), ratio: (1, 1)\n",
      "train loader: FlashLoader(\n",
      "\tData Path: /n/alvarez_lab_tier1/Users/nblauch/datasets/ffcv/imagenet/train_compressed.ffcv\n",
      "\tBatch Size: 128\n",
      "\tOrder: OrderOption.QUASI_RANDOM\n",
      "\tNumber of Workers: 11\n",
      "\tOS Cache: 1\n",
      "\tDistributed: 0\n",
      "\tDrop Last: True\n",
      "\tRecompile: False\n",
      "\tAfter Batch Pipelines:\n",
      " {'image': Compose(\n",
      "    ToTorchImage(device=cuda, dtype=torch.float32, from_numpy=True)\n",
      "    RandomHorizontalFlip(p=0.5, seed=None)\n",
      ")}\n",
      ")\n",
      "val loader crop ratio: 1.0\n",
      "val loader: FlashLoader(\n",
      "\tData Path: /n/alvarez_lab_tier1/Users/nblauch/datasets/ffcv/imagenet/val_compressed.ffcv\n",
      "\tBatch Size: 128\n",
      "\tOrder: OrderOption.SEQUENTIAL\n",
      "\tNumber of Workers: 11\n",
      "\tOS Cache: True\n",
      "\tDistributed: 0\n",
      "\tDrop Last: False\n",
      "\tRecompile: False\n",
      "\tAfter Batch Pipelines:\n",
      " {'image': Compose(\n",
      "    ToTorchImage(device=cuda, dtype=torch.float32, from_numpy=True)\n",
      "    NormalizeGPU(mean=tensor([0.4850, 0.4560, 0.4060], device='cuda:0', dtype=torch.float64), std=tensor([0.2290, 0.2240, 0.2250], device='cuda:0', dtype=torch.float64), inplace=True)\n",
      ")}\n",
      ")\n",
      "NUM TRAINING EXAMPLES: 1281167\n",
      "=> Logging in /n/netscratch/alvarez_lab/Everyone/nblauch/saccadenet/logs/XxepnJ0AQo\n",
      "HydraConfig was not set\n",
      "skipping hydra directory copying\n",
      "Training backbone: True\n"
     ]
    }
   ],
   "source": [
    "from foveation import get_trainer_from_base_fn\n",
    "\n",
    "base_fn = 'fovknnalexnet_a-1_res-64_in1k'\n",
    "# edit the paths to those storing your ImageNet-1K FFCV files\n",
    "# in general, any kwarg you pass in will be used to update the loaded config file\n",
    "kwargs = {\n",
    "    'data.train_dataset': '/n/alvarez_lab_tier1/Users/nblauch/datasets/ffcv/imagenet/train_compressed.ffcv',\n",
    "    'data.val_dataset': '/n/alvarez_lab_tier1/Users/nblauch/datasets/ffcv/imagenet/val_compressed.ffcv',\n",
    "          }\n",
    "trainer = get_trainer_from_base_fn(base_fn, load=True, model_dirs=['../models'], **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d06ca4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/391 [00:00<01:16,  5.09it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs, activations, targets = trainer.compute_activations(trainer.val_loader, layer_names=['backbone.layers.3', 'backbone', 'projector'], max_batches=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db8c1b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'backbone.layers.3': (512, 4, 384, 60),\n",
       " 'backbone': (512, 4, 256),\n",
       " 'projector': (512, 4, 1024)}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v.shape for k, v in activations.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86367c8f",
   "metadata": {},
   "source": [
    "note that we also now have the network outputs, which have aggregated over fixations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d79b820f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 1000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a916a2",
   "metadata": {},
   "source": [
    "we can quickly check our top-1 accuracy (note: this is an unstable estimate since we used a small number of batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed4a521c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5781, device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.val_meters['top_1_val'](torch.tensor(outputs), torch.tensor(targets))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knnconv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
